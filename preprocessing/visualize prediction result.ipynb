{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-51f40407cab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from scipy import sparse\n",
    "import models\n",
    "import data\n",
    "import metric\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Variational Autoencoders for Collaborative Filtering')\n",
    "parser.add_argument('--data', type=str, default='ml-20m',\n",
    "                    help='Movielens-20m dataset location')\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--wd', type=float, default=0.00,\n",
    "                    help='weight decay coefficient')\n",
    "parser.add_argument('--batch_size', type=int, default=500,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--total_anneal_steps', type=int, default=200000,\n",
    "                    help='the total number of gradient updates for annealing')\n",
    "parser.add_argument('--anneal_cap', type=float, default=0.2,\n",
    "                    help='largest annealing parameter')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save', type=str, default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--train_test', type=str, default='train')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse2torch_sparse(data):\n",
    "    \"\"\"\n",
    "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
    "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
    "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
    "    \"\"\"\n",
    "    samples = data.shape[0]\n",
    "    features = data.shape[1]\n",
    "    coo_data = data.tocoo()\n",
    "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
    "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
    "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
    "    values = np.array([row2val[r] for r in coo_data.row])\n",
    "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
    "    return t\n",
    "\n",
    "def naive_sparse2tensor(data):\n",
    "    return torch.FloatTensor(data.toarray())\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    global update_count\n",
    "\n",
    "    np.random.shuffle(idxlist)\n",
    "    \n",
    "    for batch_idx, start_idx in enumerate(range(0, N, args.batch_size)):\n",
    "        end_idx = min(start_idx + args.batch_size, N)\n",
    "        data = train_data[idxlist[start_idx:end_idx]]\n",
    "        data = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "        if args.total_anneal_steps > 0:\n",
    "            anneal = min(args.anneal_cap, \n",
    "                            1. * update_count / args.total_anneal_steps)\n",
    "        else:\n",
    "            anneal = args.anneal_cap\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
    "                    'loss {:4.2f}'.format(\n",
    "                        epoch, batch_idx, len(range(0, N, args.batch_size)),\n",
    "                        elapsed * 1000 / args.log_interval,\n",
    "                        train_loss / args.log_interval))\n",
    "            \n",
    "            # Log loss to tensorboard\n",
    "            n_iter = (epoch - 1) * len(range(0, N, args.batch_size)) + batch_idx\n",
    "            writer.add_scalars('data/loss', {'train': train_loss / args.log_interval}, n_iter)\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_loss = 0.0\n",
    "\n",
    "def evaluate(data_tr, data_te):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n100_list = []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "    \n",
    "    total_item = []\n",
    "    correct_item = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, e_N, args.batch_size):\n",
    "            end_idx = min(start_idx + args.batch_size, N)\n",
    "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "            data_tensor = naive_sparse2tensor(data).to(device)\n",
    "\n",
    "            if args.total_anneal_steps > 0:\n",
    "                anneal = min(args.anneal_cap, \n",
    "                               1. * update_count / args.total_anneal_steps)\n",
    "            else:\n",
    "                anneal = args.anneal_cap\n",
    "\n",
    "            recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "            loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Exclude examples from training set\n",
    "            recon_batch = recon_batch.cpu().numpy()\n",
    "            recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "            n100 = metric.NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
    "            r20, total_col_index_batch, correct_col_index_batch = metric.Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
    "            r50, _, _ = metric.Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
    "\n",
    "            total_col_index_batch = list(total_col_index_batch)\n",
    "            correct_col_index_batch = list(correct_col_index_batch)\n",
    "\n",
    "            print(\"total_col_index_batch\", len(total_col_index_batch))\n",
    "            print(\"correct_col_index_batch\", len(correct_col_index_batch))\n",
    "\n",
    "            total_item += total_col_index_batch\n",
    "            correct_item += correct_col_index_batch\n",
    "\n",
    "            # print(\"n100\", n100)\n",
    "            # print(\"r20\", r20)\n",
    "            # print(\"r50\", r50)\n",
    "\n",
    "            n100_list.append(n100)\n",
    "            r20_list.append(r20)\n",
    "            r50_list.append(r50)\n",
    " \n",
    "    total_freq_map = collections.Counter(total_item)\n",
    "    total_freq_map = dict(total_freq_map)\n",
    "\n",
    "    correct_freq_map = collections.Counter(correct_item)\n",
    "    correct_freq_map = dict(correct_freq_map)\n",
    "\n",
    "    correct_ratio_map = dict()\n",
    "\n",
    "    for itemid in total_freq_map:\n",
    "        total_item_freq = total_freq_map[itemid]\n",
    "\n",
    "        correct_item_freq = 0\n",
    "        if itemid in correct_freq_map:  \n",
    "            correct_item_freq = correct_freq_map[itemid]\n",
    "        correct_ratio = correct_item_freq/total_item_freq\n",
    "\n",
    "        correct_ratio_map[itemid] = correct_ratio\n",
    "\n",
    "    total_loss /= len(range(0, e_N, args.batch_size))\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, np.mean(n100_list), np.mean(r20_list), np.mean(r50_list), correct_ratio_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproductibility.\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "loader = data.DataLoader(args.data)\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "p_dims = [200, 600, n_items]\n",
    "model = models.MultiVAE(p_dims).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
    "criterion = models.loss_function\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "# TensorboardX Writer\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "best_n100 = -np.inf\n",
    "update_count = 0\n",
    "\n",
    "train_test_flag = args.train_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "if train_test_flag == \"train\":\n",
    "    try:\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            train()\n",
    "            val_loss, n100, r20, r50 = evaluate(vad_data_tr, vad_data_te)\n",
    "            print('-' * 89)\n",
    "            print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
    "                    'n100 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
    "                        epoch, time.time() - epoch_start_time, val_loss,\n",
    "                        n100, r20, r50))\n",
    "            print('-' * 89)\n",
    "\n",
    "            n_iter = epoch * len(range(0, N, args.batch_size))\n",
    "            writer.add_scalars('data/loss', {'valid': val_loss}, n_iter)\n",
    "            writer.add_scalar('data/n100', n100, n_iter)\n",
    "            writer.add_scalar('data/r20', r20, n_iter)\n",
    "            writer.add_scalar('data/r50', r50, n_iter)\n",
    "\n",
    "            # Save the model if the n100 is the best we've seen so far.\n",
    "            if n100 > best_n100:\n",
    "                with open(args.save, 'wb') as f:\n",
    "                    torch.save(model, f)\n",
    "                best_n100 = n100\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 89)\n",
    "        print('Exiting from training early')\n",
    "else:\n",
    "    # Load the best saved model.\n",
    "\n",
    "    print(\"++\"*10, \"testing\", \"++\"*10)\n",
    "\n",
    "    with open(args.save, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "\n",
    "    # Run on test data.\n",
    "    test_loss, n100, r20, r50, correct_ratio_map = evaluate(test_data_tr, test_data_te)\n",
    "    print('=' * 89)\n",
    "    print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r20 {:4.2f} | '\n",
    "            'r50 {:4.2f}'.format(test_loss, n100, r20, r50))\n",
    "    print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
